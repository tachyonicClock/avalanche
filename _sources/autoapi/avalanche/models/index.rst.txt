:mod:`avalanche.models`
=======================

.. py:module:: avalanche.models

.. autoapi-nested-parse::

   The :py:mod:`models` module provides a set of (eventually pre-trained) models
   that can be used for your continual learning experiments and applications.
   These models are mostly `torchvision.models
   <https://pytorch.org/vision/0.8/models.html#torchvision-models>`_ and `pytorchcv
   <https://pypi.org/project/pytorchcv/>`_ but we plan to add more architectures in
   the near future.



Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   batch_renorm/index.rst
   dynamic_modules/index.rst
   dynamic_optimizers/index.rst
   icarl_resnet/index.rst
   mlp_tiny_imagenet/index.rst
   mobilenetv1/index.rst
   ncm_classifier/index.rst
   pnn/index.rst
   pytorchcv_wrapper/index.rst
   simple_cnn/index.rst
   simple_mlp/index.rst
   slda_resnet/index.rst
   utils/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.models.SimpleCNN
   avalanche.models.MTSimpleCNN
   avalanche.models.SimpleMLP
   avalanche.models.MTSimpleMLP
   avalanche.models.SimpleMLP_TinyImageNet
   avalanche.models.MobilenetV1
   avalanche.models.DynamicModule
   avalanche.models.MultiTaskModule
   avalanche.models.IncrementalClassifier
   avalanche.models.MultiHeadClassifier
   avalanche.models.TrainEvalModel
   avalanche.models.FeatureExtractorBackbone
   avalanche.models.SLDAResNetModel
   avalanche.models.IcarlNet
   avalanche.models.NCMClassifier



Functions
~~~~~~~~~

.. autoapisummary::

   avalanche.models.avalanche_forward
   avalanche.models.initialize_icarl_net
   avalanche.models.make_icarl_net


.. py:class:: SimpleCNN(num_classes=10)

   Bases: :class:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super(Model, self).__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   Initializes internal Module state, shared by both nn.Module and ScriptModule.

   .. method:: forward(self, x)



.. py:class:: MTSimpleCNN

   Bases: :class:`avalanche.models.simple_cnn.SimpleCNN`, :class:`avalanche.models.dynamic_modules.MultiTaskModule`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super(Model, self).__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   Multi-task CNN with multi-head classifier.

   .. method:: forward(self, x, task_labels)

      compute the output given the input `x` and task labels.

      :param x:
      :param task_labels: task labels for each sample.
      :return:



.. py:class:: SimpleMLP(num_classes=10, input_size=28 * 28, hidden_size=512, hidden_layers=1)

   Bases: :class:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super(Model, self).__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   Initializes internal Module state, shared by both nn.Module and ScriptModule.

   .. method:: forward(self, x)



.. py:class:: MTSimpleMLP(input_size=28 * 28, hidden_size=512)

   Bases: :class:`avalanche.models.dynamic_modules.MultiTaskModule`

   Multi-task modules are `torch.nn.Modules`s for multi-task
   scenarios. The `forward` method accepts task labels, one for
   each sample in the mini-batch.

   By default the `forward` method splits the mini-batch by task
   and calls `forward_single_task`. Subclasses must implement
   `forward_single_task` or override `forward.

   Multi-task MLP with multi-head classifier.

   .. method:: forward(self, x, task_labels)

      compute the output given the input `x` and task labels.

      :param x:
      :param task_labels: task labels for each sample.
      :return:



.. py:class:: SimpleMLP_TinyImageNet(num_classes=200, num_channels=3)

   Bases: :class:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super(Model, self).__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   Initializes internal Module state, shared by both nn.Module and ScriptModule.

   .. method:: forward(self, x)



.. py:class:: MobilenetV1(pretrained=True, latent_layer_num=20)

   Bases: :class:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super(Model, self).__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   Initializes internal Module state, shared by both nn.Module and ScriptModule.

   .. method:: forward(self, x, latent_input=None, return_lat_acts=False)



.. py:class:: DynamicModule

   Bases: :class:`torch.nn.Module`

   Dynamic Modules are Avalanche modules that can be incrementally
   expanded to allow architectural modifications (multi-head
   classifiers, progressive networks, ...).

   Compared to pytoch Modules, they provide an additional method,
   `model_adaptation`, which adapts the model given data from the
   current experience.

   Initializes internal Module state, shared by both nn.Module and ScriptModule.

   .. method:: adaptation(self, dataset: AvalancheDataset = None)

      Adapt the module (freeze units, add units...) using the current
      data. Optimizers must be updated after the model adaptation.

      Avalanche strategies call this method to adapt the architecture
      *before* processing each experience. Strategies also update the
      optimizer automatically.

      .. warning::
          As a general rule, you should NOT use this method to train the
          model. The dataset should be used only to check conditions which
          require the model's adaptation, such as the discovery of new
          classes or tasks.

      :param dataset: data from the current experience.
      :return:


   .. method:: train_adaptation(self, dataset: AvalancheDataset)

      Module's adaptation at training time.

      Avalanche strategies automatically call this method *before* training
      on each experience.


   .. method:: eval_adaptation(self, dataset: AvalancheDataset)

      Module's adaptation at evaluation time.

      Avalanche strategies automatically call this method *before* evaluating
      on each experience.

      .. warning::
          This method receives the experience's data at evaluation time
          because some dynamic models need it for adaptation. For example,
          an incremental classifier needs to be expanded even at evaluation
          time if new classes are available. However, you should **never**
          use this data to **train** the module's parameters.



.. py:class:: MultiTaskModule

   Bases: :class:`torch.nn.Module`

   Multi-task modules are `torch.nn.Modules`s for multi-task
   scenarios. The `forward` method accepts task labels, one for
   each sample in the mini-batch.

   By default the `forward` method splits the mini-batch by task
   and calls `forward_single_task`. Subclasses must implement
   `forward_single_task` or override `forward.

   Initializes internal Module state, shared by both nn.Module and ScriptModule.

   .. method:: forward(self, x: torch.Tensor, task_labels: torch.Tensor) -> torch.Tensor

      compute the output given the input `x` and task labels.

      :param x:
      :param task_labels: task labels for each sample.
      :return:


   .. method:: forward_single_task(self, x: torch.Tensor, task_label: int) -> torch.Tensor
      :abstractmethod:

      compute the output given the input `x` and task label.

      :param x:
      :param task_label: a single task label.
      :return:



.. py:class:: IncrementalClassifier(in_features, initial_out_features=2)

   Bases: :class:`avalanche.models.dynamic_modules.DynamicModule`

   Dynamic Modules are Avalanche modules that can be incrementally
   expanded to allow architectural modifications (multi-head
   classifiers, progressive networks, ...).

   Compared to pytoch Modules, they provide an additional method,
   `model_adaptation`, which adapts the model given data from the
   current experience.

   Output layer that incrementally adds units whenever new classes are
   encountered.

   Typically used in class-incremental benchmarks where the number of
   classes grows over time.

   :param in_features: number of input features.
   :param initial_out_features: initial number of classes (can be
       dynamically expanded).

   .. method:: adaptation(self, dataset: AvalancheDataset)

      If `dataset` contains unseen classes the classifier is expanded.

      :param dataset: data from the current experience.
      :return:


   .. method:: forward(self, x, **kwargs)

      compute the output given the input `x`. This module does not use
      the task label.

      :param x:
      :return:



.. py:class:: MultiHeadClassifier(in_features, initial_out_features=2)

   Bases: :class:`avalanche.models.dynamic_modules.MultiTaskModule`, :class:`avalanche.models.dynamic_modules.DynamicModule`

   Multi-task modules are `torch.nn.Modules`s for multi-task
   scenarios. The `forward` method accepts task labels, one for
   each sample in the mini-batch.

   By default the `forward` method splits the mini-batch by task
   and calls `forward_single_task`. Subclasses must implement
   `forward_single_task` or override `forward.

   Multi-head classifier with separate heads for each task.

   Typically used in task-incremental benchmarks where task labels are
   available and provided to the model.

   .. note::
       Each output head may have a different shape, and the number of
       classes can be determined automatically.

       However, since pytorch doest not support jagged tensors, when you
       compute a minibatch's output you must ensure that each sample
       has the same output size, otherwise the model will fail to
       concatenate the samples together.

       These can be easily ensured in two possible ways:
       - each minibatch contains a single task, which is the case in most
           common benchmarks in Avalanche. Some exceptions to this setting
           are multi-task replay or cumulative strategies.
       - each head has the same size, which can be enforced by setting a
           large enough `initial_out_features`.

   :param in_features: number of input features.
   :param initial_out_features: initial number of classes (can be
       dynamically expanded).

   .. method:: adaptation(self, dataset: AvalancheDataset)

      If `dataset` contains new tasks, a new head is initialized.

      :param dataset: data from the current experience.
      :return:


   .. method:: forward_single_task(self, x, task_label)

      compute the output given the input `x`. This module uses the task
      label to activate the correct head.

      :param x:
      :param task_label:
      :return:



.. py:class:: TrainEvalModel(feature_extractor, train_classifier, eval_classifier)

   Bases: :class:`avalanche.models.dynamic_modules.DynamicModule`

   TrainEvalModel.
   This module allows to wrap together a common feature extractor and
   two classifiers: one used during training time and another
   used at test time. The classifier is switched when `self.adaptation()`
   is called.

   :param feature_extractor: a differentiable feature extractor
   :param train_classifier: a differentiable classifier used
       during training
   :param eval_classifier: a classifier used during testing.
       Doesn't have to be differentiable.

   .. method:: forward(self, x)


   .. method:: train_adaptation(self, dataset: AvalancheDataset = None)

      Module's adaptation at training time.

      Avalanche strategies automatically call this method *before* training
      on each experience.


   .. method:: eval_adaptation(self, dataset: AvalancheDataset = None)

      Module's adaptation at evaluation time.

      Avalanche strategies automatically call this method *before* evaluating
      on each experience.

      .. warning::
          This method receives the experience's data at evaluation time
          because some dynamic models need it for adaptation. For example,
          an incremental classifier needs to be expanded even at evaluation
          time if new classes are available. However, you should **never**
          use this data to **train** the module's parameters.



.. function:: avalanche_forward(model, x, task_labels)


.. py:class:: FeatureExtractorBackbone(model, output_layer_name)

   Bases: :class:`torch.nn.Module`

   This PyTorch module allows us to extract features from a backbone network
   given a layer name.

   Initializes internal Module state, shared by both nn.Module and ScriptModule.

   .. method:: forward(self, x)


   .. method:: get_name_to_module(self, model)


   .. method:: get_activation(self)


   .. method:: add_hooks(self, model)

      :param model:
      :param outputs: Outputs from layers specified in `output_layer_names`
      will be stored in `output` variable
      :param output_layer_names:
      :return:



.. py:class:: SLDAResNetModel(arch='resnet18', output_layer_name='layer4.1', imagenet_pretrained=True, device='cpu')

   Bases: :class:`torch.nn.Module`

   This is a model wrapper to reproduce experiments from the original
   paper of Deep Streaming Linear Discriminant Analysis by using
   a pretrained ResNet model.

   :param arch: backbone architecture (default is resnet-18, but others
   can be used by modifying layer for
   feature extraction in `self.feature_extraction_wrapper'
   :param imagenet_pretrained: True if initializing backbone with imagenet
   pre-trained weights else False
   :param output_layer_name: name of the layer from feature extractor
   :param device: cpu, gpu or other device

   .. method:: pool_feat(features)
      :staticmethod:


   .. method:: forward(self, x)

      :param x: raw x data



.. function:: initialize_icarl_net(m: Module)


.. function:: make_icarl_net(num_classes: int, n=5, c=3) -> IcarlNet


.. py:class:: IcarlNet(num_classes: int, n=5, c=3)

   Bases: :class:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super(Model, self).__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   Initializes internal Module state, shared by both nn.Module and ScriptModule.

   .. method:: forward(self, x)



.. py:class:: NCMClassifier(class_mean=None)

   Bases: :class:`torch.nn.Module`

   NCM Classifier.
   NCMClassifier performs nearest class mean classification
   measuring the distance between the input tensor and the
   ones stored in 'self.class_means'.

   :param class_mean: tensor of dimension (num_classes x feature_size)
       used to classify input patterns.

   .. method:: forward(self, x)



