:mod:`avalanche.benchmarks.datasets`
====================================

.. py:module:: avalanche.benchmarks.datasets


Subpackages
-----------
.. toctree::
   :titlesonly:
   :maxdepth: 3

   core50/index.rst
   cub200/index.rst
   inaturalist/index.rst
   mini_imagenet/index.rst
   openloris/index.rst
   stream51/index.rst
   tiny_imagenet/index.rst


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   dataset_utils/index.rst
   downloadable_dataset/index.rst
   imagenet_data/index.rst
   omniglot/index.rst
   torchvision_wrapper/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.benchmarks.datasets.DownloadableDataset
   avalanche.benchmarks.datasets.SimpleDownloadableDataset
   avalanche.benchmarks.datasets.CORe50Dataset
   avalanche.benchmarks.datasets.CUB200
   avalanche.benchmarks.datasets.MiniImageNetDataset
   avalanche.benchmarks.datasets.OpenLORIS
   avalanche.benchmarks.datasets.TinyImagenet
   avalanche.benchmarks.datasets.Omniglot
   avalanche.benchmarks.datasets.Stream51
   avalanche.benchmarks.datasets.INATURALIST2018



Functions
~~~~~~~~~

.. autoapisummary::

   avalanche.benchmarks.datasets.get_default_dataset_location
   avalanche.benchmarks.datasets.MNIST
   avalanche.benchmarks.datasets.FashionMNIST
   avalanche.benchmarks.datasets.KMNIST
   avalanche.benchmarks.datasets.EMNIST
   avalanche.benchmarks.datasets.QMNIST
   avalanche.benchmarks.datasets.FakeData
   avalanche.benchmarks.datasets.CocoCaptions
   avalanche.benchmarks.datasets.CocoDetection
   avalanche.benchmarks.datasets.LSUN
   avalanche.benchmarks.datasets.LSUN
   avalanche.benchmarks.datasets.ImageFolder
   avalanche.benchmarks.datasets.DatasetFolder
   avalanche.benchmarks.datasets.ImageNet
   avalanche.benchmarks.datasets.CIFAR10
   avalanche.benchmarks.datasets.CIFAR100
   avalanche.benchmarks.datasets.STL10
   avalanche.benchmarks.datasets.SVHN
   avalanche.benchmarks.datasets.PhotoTour
   avalanche.benchmarks.datasets.SBU
   avalanche.benchmarks.datasets.Flickr8k
   avalanche.benchmarks.datasets.Flickr30k
   avalanche.benchmarks.datasets.VOCDetection
   avalanche.benchmarks.datasets.VOCSegmentation
   avalanche.benchmarks.datasets.Cityscapes
   avalanche.benchmarks.datasets.SBDataset
   avalanche.benchmarks.datasets.USPS
   avalanche.benchmarks.datasets.Kinetics400
   avalanche.benchmarks.datasets.HMDB51
   avalanche.benchmarks.datasets.UCF101
   avalanche.benchmarks.datasets.CelebA


.. function:: get_default_dataset_location(dataset_name: str) -> Path

   Return the default location of a dataset.

   This currently returns "~/.avalanche/data/<dataset_name>" but in the future
   an environment variable bay be introduced to change the root path.

   :param dataset_name: The name of the dataset. Consider using a string that
       can be used to name a directory in most filesystems!
   :return: The default path for the dataset.


.. py:class:: DownloadableDataset(root: Union[str, Path], download: bool = True, verbose: bool = False)

   Bases: :class:`Dataset[T_co]`, :class:`abc.ABC`

   Base class for a downloadable dataset.

   It is recommended to extend this class if a dataset can be downloaded from
   the internet. This implementation codes the recommended behaviour for
   downloading and verifying the dataset.

   The dataset child class must implement the `_download_dataset`,
   `_load_metadata` and `_download_error_message` methods

   The child class, in its constructor, must call the already implemented
   `_load_dataset` method (otherwise nothing will happen).

   A further simplification can be obtained by extending
   :class:`SimpleDownloadableDataset` instead of this class.
   :class:`SimpleDownloadableDataset` is recommended if a single archive is to
   be downloaded and extracted to the root folder "as is".

   The standardized procedure coded by `_load_dataset` is as follows:

   - First, `_load_metadata` is called to check if the dataset can be correctly
     loaded at the `root` path. This method must check if the data found
     at the `root` path is correct and that metadata can be correctly loaded.
     If this method succeeds (by returning True) the process is completed.
   - If `_load_metadata` fails (by returning False or by raising an error),
     then a download will be attempted if the download flag was set to True.
     The download must be implemented in `_download_dataset`. The
     procedure can be drastically simplified by using the `_download_file`,
     `_extract_archive` and `_download_and_extract_archive` helpers.
   - If the download succeeds (doesn't raise an error), then `_load_metadata`
     will be called again.

   If an error occurs, the `_download_error_message` will be called to obtain
   a message (a string) to show to the user. That message should contain
   instructions on how to download and prepare the dataset manually.

   Creates an instance of a downloadable dataset.

   Consider looking at the class documentation for the precise details on
   how to extend this class.

   Beware that calling this constructor only fills the `root` field. The
   download and metadata loading procedures are triggered only by
   calling `_load_dataset`.

   :param root: The root path where the dataset will be downloaded.
       Consider passing a path obtained by calling
       `get_default_dataset_location` with the name of the dataset.
   :param download: If True, the dataset will be downloaded if needed.
       If False and the dataset can't be loaded from the provided root
       path, an error will be raised when calling the `_load_dataset`
       method. Defaults to True.
   :param verbose: If True, some info about the download process will be
       printed. Defaults to False.

   .. attribute:: root
      :annotation: :Path

      The path to the dataset.


   .. attribute:: download
      :annotation: :bool

      If True, the dataset will be downloaded (only if needed).


   .. attribute:: verbose
      :annotation: :bool

      If True, some info about the download process will be printed.



.. py:class:: SimpleDownloadableDataset(root_or_dataset_name: str, url: str, checksum: Optional[str], download: bool = False, verbose: bool = False)

   Bases: :class:`DownloadableDataset[T_co]`, :class:`abc.ABC`

   Base class for a downloadable dataset consisting of a single archive file.

   It is recommended to extend this class if a dataset can be downloaded from
   the internet as a single archive. For multi-file implementation or if
   a more fine-grained control is required, consider extending
   :class:`DownloadableDataset` instead.

   This is a simplified version of :class:`DownloadableDataset` where the
   following assumptions must hold:
   - The dataset is made of a single archive.
   - The archive must be extracted to the root folder "as is" (which means
       that no subdirectories must be created).

   The child class is only required to extend the `_load_metadata` method,
   which must check the dataset integrity and load the dataset metadata.

   Apart from that, the same assumptions of :class:`DownloadableDataset` hold.
   Remember to call the `_load_dataset` method in the child class constructor.

   Creates an instance of a simple downloadable dataset.

   Consider looking at the class documentation for the precise details on
   how to extend this class.

   Beware that calling this constructor only fills the `root` field. The
   download and metadata loading procedures are triggered only by
   calling `_load_dataset`.

   :param root_or_dataset_name: The root path where the dataset will be
       downloaded. If a directory name is passed, then the root obtained by
       calling `get_default_dataset_location` will be used (recommended).
       To check if this parameter is a path, the constructor will check if
       it contains the '' or '/' characters or if it is a Path instance.
   :param url: The url of the archive.
   :param checksum: The MD5 hash to use when verifying the downloaded
       archive. Can be None, in which case the check will be skipped.
       It is recommended to always fill this parameter.
   :param download: If True, the dataset will be downloaded if needed.
       If False and the dataset can't be loaded from the provided root
       path, an error will be raised when calling the `_load_dataset`
       method. Defaults to False.
   :param verbose: If True, some info about the download process will be
       printed. Defaults to False.


.. py:class:: CORe50Dataset(root: Union[str, Path] = get_default_dataset_location('core50'), *, train=True, transform=None, target_transform=None, loader=default_loader, download=True, mini=False, object_level=True)

   Bases: :class:`avalanche.benchmarks.datasets.downloadable_dataset.DownloadableDataset`

   CORe50 Pytorch Dataset 

   Creates an instance of the CORe50 dataset.

   :param root: root for the datasets data.
   :param train: train or test split.
   :param transform: eventual transformations to be applied.
   :param target_transform: eventual transformation to be applied to the
       targets.
   :param loader: the procedure to load the instance from the storage.
   :param download: boolean to automatically download data. Default to
       True.
   :param mini: boolean to use the 32x32 version instead of the 128x128.
       Default to False.
   :param object_level: if the classification is objects based or
       category based: 50 or 10 way classification problem. Default to True
       (50-way object classification problem)

   .. method:: __getitem__(self, index)

      Args:
          index (int): Index

      Returns:
          tuple: (sample, target) where target is class_index of the target
              class.


   .. method:: __len__(self)



.. py:class:: CUB200(root: Union[str, Path] = get_default_dataset_location('CUB_200_2011'), *, train=True, transform=None, target_transform=None, loader=default_loader, download=True)

   Bases: :class:`avalanche.benchmarks.utils.PathsDataset`, :class:`avalanche.benchmarks.datasets.DownloadableDataset`

   Basic CUB200 PathsDataset to be used as a standard PyTorch Dataset.
   A classic continual learning benchmark built on top of this dataset
   can be found in 'benchmarks.classic', while for more custom benchmark
   design please use the 'benchmarks.generators'.

   :param root: root dir where the dataset can be found or downloaded.
       Default to '~/.avalanche/data/CUB_200_2011'.
   :param train: train or test subset of the original dataset. Default
       to True.
   :param transform: eventual input data transformations to apply.
       Default to None.
   :param target_transform: eventual target data transformations to apply.
       Default to None.
   :param loader: method to load the data from disk. Default to
       torchvision default_loader.
   :param download: default set to True. If the data is already
       downloaded it will skip the download.

   .. attribute:: images_folder
      :annotation: = CUB_200_2011/images

      

   .. attribute:: official_url
      :annotation: = http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz

      

   .. attribute:: gdrive_url
      :annotation: = https://drive.google.com/u/0/uc?id=1hbzc_P1FuxMkcabkgn9ZKinBwW683j45

      

   .. attribute:: filename
      :annotation: = CUB_200_2011.tgz

      

   .. attribute:: tgz_md5
      :annotation: = 97eceeb196236b17998738112f37df78

      


.. py:class:: MiniImageNetDataset(imagenet_path: Union[str, Path], split: Literal['all', 'train', 'val', 'test'] = 'all', resize_to: Union[int, Tuple[int, int]] = 84, loader=default_loader)

   Bases: :class:`torch.utils.data.dataset.Dataset`

   The MiniImageNet dataset.

   This implementation is based on the one from
   https://github.com/yaoyao-liu/mini-imagenet-tools. Differently from that,
   this class doesn't rely on a pre-generated mini imagenet folder. Instead,
   this will use the original ImageNet folder by resizing images on-the-fly.

   The list of included files are the ones defined in the CSVs taken from the
   aforementioned repository. Those CSVs are generated by Ravi and Larochelle.
   See the linked repository for more details.

   Exactly as happens with the torchvision :class:`ImageNet` class, textual
   class labels (wnids) such as "n02119789", "n02102040", etc. are mapped to
   numerical labels based on their ascending order.

   All the fields found in the torchvision implementation of the ImageNet
   dataset (`wnids`, `wnid_to_idx`, `classes`, `class_to_idx`) are available.

   Creates an instance of the Mini ImageNet dataset.

   This dataset allows to obtain the whole dataset or even only specific
   splits. Beware that, when using a split different that "all", the
   returned dataset will contain patterns of a subset of the 100 classes.
   This happens because MiniImagenet was created with the idea of training,
   validating and testing on a disjoint set of classes.

   This implementation uses the filelists provided by
   https://github.com/yaoyao-liu/mini-imagenet-tools, which are the ones
   generated by Ravi and Larochelle (see the linked repo for more details).

   :param imagenet_path: The path to the imagenet folder. This has to be
       the path to the full imagenet 2012 folder (plain, not resized).
       Only the "train" folder will be used. Because of this, passing the
       path to the imagenet 2012 "train" folder is also allowed.
   :param split: The split to obtain. Defaults to "all". Valid values are
       "all", "train", "val" and "test".
   :param resize_to: The size of the output images. Can be an `int` value
       or a tuple of two ints. When passing a single `int` value, images
       will be resized by forcing as 1:1 aspect ratio. Defaults to 84,
       which means that images will have size 84x84.

   .. attribute:: imagenet_path
      

      The path to the "train" folder of full imagenet 2012 directory.


   .. attribute:: split
      :annotation: :Literal['all', 'train', 'val', 'test']

      The required split.


   .. attribute:: resize_to
      :annotation: :Tuple[int, int]

      The size of the output images, as a two ints tuple.


   .. attribute:: image_paths
      :annotation: :List[str] = []

      The paths to images.


   .. attribute:: targets
      :annotation: :List[int] = []

      The class labels for the patterns. Aligned with the image_paths field.


   .. attribute:: wnids
      :annotation: :List[str] = []

      The list of wnids (the textual class labels, such as "n02119789").


   .. attribute:: wnid_to_idx
      :annotation: :Dict[str, int]

      A dictionary mapping wnids to numerical labels in range [0, 100).


   .. attribute:: classes
      :annotation: :List[Tuple[str, ...]] = []

      A list mapping numerical labels (the element index) to a tuple of human
      readable categories. For instance:
      ('great grey owl', 'great gray owl', 'Strix nebulosa').


   .. attribute:: class_to_idx
      :annotation: :Dict[str, int]

      A dictionary mapping each string of the tuples found in the classes 
      field to their numerical label. That is, this dictionary contains the 
      inverse mapping of classes field.


   .. method:: get_train_path(root_path: Union[str, Path])
      :staticmethod:


   .. method:: prepare_dataset(self)


   .. method:: __len__(self)


   .. method:: __getitem__(self, item)



.. py:class:: OpenLORIS(root=get_default_dataset_location('openloris'), *, train=True, transform=None, target_transform=None, loader=default_loader, download=True)

   Bases: :class:`avalanche.benchmarks.datasets.DownloadableDataset`

   OpenLORIS Pytorch Dataset 

   Creates an instance of a downloadable dataset.

   Consider looking at the class documentation for the precise details on
   how to extend this class.

   Beware that calling this constructor only fills the `root` field. The
   download and metadata loading procedures are triggered only by
   calling `_load_dataset`.

   :param root: The root path where the dataset will be downloaded.
       Consider passing a path obtained by calling
       `get_default_dataset_location` with the name of the dataset.
   :param download: If True, the dataset will be downloaded if needed.
       If False and the dataset can't be loaded from the provided root
       path, an error will be raised when calling the `_load_dataset`
       method. Defaults to True.
   :param verbose: If True, some info about the download process will be
       printed. Defaults to False.

   .. method:: __getitem__(self, index)


   .. method:: __len__(self)



.. py:class:: TinyImagenet(root: Union[str, Path] = get_default_dataset_location('tinyimagenet'), *, train: bool = True, transform=None, target_transform=None, loader=default_loader, download=True)

   Bases: :class:`avalanche.benchmarks.datasets.SimpleDownloadableDataset`

   Tiny Imagenet Pytorch Dataset

   Creates an instance of the Tiny Imagenet dataset.

   :param root: folder in which to download dataset.
   :param train: True for training set, False for test set.
   :param transform: Pytorch transformation function for x.
   :param target_transform: Pytorch transformation function for y.
   :param loader: the procedure to load the instance from the storage.
   :param bool download: If True, the dataset will be  downloaded if
       needed.

   .. attribute:: filename
      :annotation: = ['tiny-imagenet-200.zip', 'http://cs231n.stanford.edu/tiny-imagenet-200.zip']

      

   .. attribute:: md5
      :annotation: = 90528d7ca1a48142e341f4ef8d21d0de

      

   .. method:: labels2dict(data_folder: Path)
      :staticmethod:

      Returns dictionaries to convert class names into progressive ids
      and viceversa.

      :param data_folder: The root path of tiny imagenet
      :returns: label2id, id2label: two Python dictionaries.


   .. method:: load_data(self)

      Load all images paths and targets.

      :return: train_set, test_set: (train_X_paths, train_y).


   .. method:: get_train_images_paths(self, class_name)

      Gets the training set image paths.

      :param class_name: names of the classes of the images to be
          collected.
      :returns img_paths: list of strings (paths)


   .. method:: get_test_images_paths(self, class_name)

      Gets the test set image paths

      :param class_name: names of the classes of the images to be
          collected.
      :returns img_paths: list of strings (paths)


   .. method:: __len__(self)

      Returns the length of the set 


   .. method:: __getitem__(self, index)

      Returns the index-th x, y pattern of the set 



.. py:class:: Omniglot(root: str, train: bool = True, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False)

   Bases: :class:`torchvision.datasets.Omniglot`

   Custom class used to adapt Omniglot (from Torchvision) and make it
   compatible with the Avalanche API.

   Initialize self.  See help(type(self)) for accurate signature.

   .. method:: data(self)
      :property:



.. py:class:: Stream51(root=get_default_dataset_location('stream51'), *, train=True, transform=None, target_transform=None, loader=default_loader, download=True)

   Bases: :class:`avalanche.benchmarks.datasets.DownloadableDataset`

   Stream-51 Pytorch Dataset 

   Creates an instance of a downloadable dataset.

   Consider looking at the class documentation for the precise details on
   how to extend this class.

   Beware that calling this constructor only fills the `root` field. The
   download and metadata loading procedures are triggered only by
   calling `_load_dataset`.

   :param root: The root path where the dataset will be downloaded.
       Consider passing a path obtained by calling
       `get_default_dataset_location` with the name of the dataset.
   :param download: If True, the dataset will be downloaded if needed.
       If False and the dataset can't be loaded from the provided root
       path, an error will be raised when calling the `_load_dataset`
       method. Defaults to True.
   :param verbose: If True, some info about the download process will be
       printed. Defaults to False.

   .. method:: make_dataset(data_list, ordering='class_instance', seed=666)
      :staticmethod:

      data_list
      for train: [class_id, clip_num, video_num, frame_num, bbox, file_loc]
      for test: [class_id, bbox, file_loc]


   .. method:: __getitem__(self, index)

      Args:
          index (int): Index

      Returns:
          tuple: (sample, target) where target is class_index of the target
          class.


   .. method:: __len__(self)


   .. method:: __repr__(self)

      Return repr(self).



.. function:: MNIST(*args, **kwargs)


.. function:: FashionMNIST(*args, **kwargs)


.. function:: KMNIST(*args, **kwargs)


.. function:: EMNIST(*args, **kwargs)


.. function:: QMNIST(*args, **kwargs)


.. function:: FakeData(*args, **kwargs)


.. function:: CocoCaptions(*args, **kwargs)


.. function:: CocoDetection(*args, **kwargs)


.. function:: LSUN(*args, **kwargs)


.. function:: LSUN(*args, **kwargs)


.. function:: ImageFolder(*args, **kwargs)


.. function:: DatasetFolder(*args, **kwargs)


.. function:: ImageNet(*args, **kwargs)


.. function:: CIFAR10(*args, **kwargs)


.. function:: CIFAR100(*args, **kwargs)


.. function:: STL10(*args, **kwargs)


.. function:: SVHN(*args, **kwargs)


.. function:: PhotoTour(*args, **kwargs)


.. function:: SBU(*args, **kwargs)


.. function:: Flickr8k(*args, **kwargs)


.. function:: Flickr30k(*args, **kwargs)


.. function:: VOCDetection(*args, **kwargs)


.. function:: VOCSegmentation(*args, **kwargs)


.. function:: Cityscapes(*args, **kwargs)


.. function:: SBDataset(*args, **kwargs)


.. function:: USPS(*args, **kwargs)


.. function:: Kinetics400(*args, **kwargs)


.. function:: HMDB51(*args, **kwargs)


.. function:: UCF101(*args, **kwargs)


.. function:: CelebA(*args, **kwargs)


.. py:class:: INATURALIST2018(root=expanduser('~') + '/.avalanche/data/inaturalist2018/', split='train', transform=ToTensor(), target_transform=None, loader=pil_loader, download=True, supcats=None)

   Bases: :class:`torch.utils.data.dataset.Dataset`

   INATURALIST Pytorch Dataset

   For default selection of 10 supercategories:
   - Training Images in total: 428,830
   - Validation Images in total:  23,229
   - Shape of images: torch.Size([1, 3, 600, 800])
   - Class counts per supercategory (both train/val):
    { 'Amphibia': 144,
     'Animalia': 178,
     'Arachnida': 114,
     'Aves': 1258,
     'Fungi': 321,
     'Insecta': 2031,
     'Mammalia': 234,
     'Mollusca': 262,
     'Plantae': 2917,
     'Reptilia': 284}

   Initialize self.  See help(type(self)) for accurate signature.

   .. attribute:: splits
      :annotation: = ['train', 'val', 'test']

      

   .. attribute:: def_supcats
      :annotation: = ['Amphibia', 'Animalia', 'Arachnida', 'Aves', 'Fungi', 'Insecta', 'Mammalia', 'Mollusca', 'Plantae', 'Reptilia']

      

   .. method:: __getitem__(self, index)

      Args:
          index (int): Index

      Returns:
          tuple: (sample, target) where target is class_index of the target
              class.


   .. method:: __len__(self)



